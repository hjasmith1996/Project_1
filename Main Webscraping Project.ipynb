{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "#This is the cell where I import everything used in this project. The chromedriver is in the folder. \r\n",
                "from selenium import webdriver\r\n",
                "from selenium.webdriver.common.keys import Keys\r\n",
                "import time\r\n",
                "import pandas as pd"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#List of URL home pages. These were found by a manual search. \r\n",
                "exchange_URL = 'https://en.wikipedia.org/w/index.php?title=List_of_stock_exchanges&action=history'\r\n",
                "weather_URL = 'https://www.wunderground.com/history'\r\n",
                "\r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "complete_list = []  #This list would contain all the data we would get from scraping the wikipedia page. \r\n",
                "driver.get(exchange_URL)\r\n",
                "\r\n",
                "\r\n",
                "#This link changes the amount of links per page from 50 to 500. This is great as it covers our whole range. \r\n",
                "fivehundred_link = driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[4]/a[7]')\r\n",
                "fivehundred_link.click()\r\n",
                "\r\n",
                "\r\n",
                "  #The chromedriver is selected and we are going to the stock exchange URL first. \r\n",
                "\r\n",
                "\r\n",
                "#Here is where I set the parameters \r\n",
                "edit_list = []\r\n",
                "current_year = ''\r\n",
                "current_month = ''\r\n",
                "amount_of_ranks = 16\r\n",
                "list_of_month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "full_edit_list = driver.find_element_by_xpath('//*[@id=\"pagehistory\"]') ##This is the block which contains all the links to the edited pages\r\n",
                "edit_list = full_edit_list.find_elements_by_xpath(\"*\") #This list is all the children of the full_edit_lsit\r\n",
                "completed_months_and_years = [] # A list of  two-entried tuples. It marks all the months and years which have already been scraped. \r\n",
                "change_of_format = False #At some point, there is a change in format in the wikipedia page. When this turns true, the code is thereby altered. \r\n",
                "for link in edit_list:\r\n",
                "    month_and_year = []\r\n",
                "    written_date = link.find_element_by_class_name('mw-changeslist-date') # The text here gives us the date\r\n",
                "    written_date_link = written_date.get_attribute('href')\r\n",
                "    written_date_text = written_date.text\r\n",
                "    if written_date_text == '07:40, 24 January 2021': #This link is broken. It must be skipped\r\n",
                "        continue\r\n",
                "    #The following piece of code concerns with deciphering the link's date\r\n",
                "\r\n",
                "\r\n",
                "    current_year = written_date_text[-4:]    #The last four characters of the string always indicates its year.\r\n",
                "    for month in list_of_month_names: #This goes through the actual months of the year and checks whether it is in the string or not. \r\n",
                "        if month in written_date_text:\r\n",
                "            current_month = month #If the month is in the string, then the edit was made on that month of the year.\r\n",
                "    month_and_year = (current_year, current_month) #The first entry of this list is the month of the edit and the second entry is the year.\r\n",
                "\r\n",
                "\r\n",
                "    if written_date_text == '02:17, 22 April 2021': # This is how far we are going back as bugs start arising further back in the past. \r\n",
                "        change_of_format = True\r\n",
                "    if month_and_year not in completed_months_and_years: #If the month has not been scrapped yet...This is to not have multiple datasets per month.\r\n",
                "        this_months_dictonary = {}\r\n",
                "        months_rank_list = []\r\n",
                "        months_exchange_list = []\r\n",
                "        months_location_list = []\r\n",
                "        months_market_cap_list = []\r\n",
                "        months_trade_volume_list = []\r\n",
                "        second_driver = webdriver.Chrome('./chromedriver') # It makes it easier if we use a second driver to open links to a new window. \r\n",
                "        second_driver.get(written_date_link)\r\n",
                "\r\n",
                "        #The code from here to the for loop gets the table as a list of rows. This makes it easier to scrape from \r\n",
                "        table_header = second_driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/table[2]/tbody/tr[1]')\r\n",
                "        table_header_children =table_header.find_elements_by_xpath(\"*\")\r\n",
                "        current_column = 0 \r\n",
                "        if change_of_format == False:\r\n",
                "            page_table = second_driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody')\r\n",
                "        else:\r\n",
                "            page_table = second_driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody')\r\n",
                "        list_of_rows = page_table.find_elements_by_xpath(\"*\")\r\n",
                "        for row in list_of_rows:    #For every row in the table\r\n",
                "            list_of_row_elements = row.find_elements_by_xpath(\"*\")\r\n",
                "            k = 1   #This k indicates which column we are concerned with\r\n",
                "            for row_elements in list_of_row_elements:\r\n",
                "                if len(list_of_row_elements) == 16:  #This is the correct amount of elements in a row. Any other number than this makes it difficult to scrape from. \r\n",
                "                    if k == 1:  #This column corresponds to the rank of the exchange. \r\n",
                "                        months_rank_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k ==3: #This column corresponds to the name of the exchange.\r\n",
                "                        months_exchange_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k == 6:\r\n",
                "                        split_locations = row_elements.text.split('\\n')\r\n",
                "                        months_location_list.append(split_locations)\r\n",
                "\r\n",
                "                    if k == 7:  #This column corresponds to the market's market cap. \r\n",
                "                        months_market_cap_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k ==8: # This column corresponds to the market's trade volume\r\n",
                "                        months_trade_volume_list.append(row_elements.text)\r\n",
                "                k += 1 #  Move to the next column \r\n",
                "                \r\n",
                "                    \r\n",
                "        #Here we are adding the information we have scrapped to this months dictionary. \r\n",
                "        this_months_dictonary['Rank'] = months_rank_list\r\n",
                "        this_months_dictonary['Name of exchange'] = months_exchange_list\r\n",
                "        this_months_dictonary['Locations'] = months_location_list\r\n",
                "        this_months_dictonary['Market Cap'] = months_market_cap_list\r\n",
                "        this_months_dictonary['Trade Voume'] = months_trade_volume_list\r\n",
                "        complete_list.append([month_and_year, this_months_dictonary])                      \r\n",
                "        second_driver.quit()\r\n",
                "        completed_months_and_years.append(month_and_year)\r\n",
                "    \r\n",
                "    if written_date_text == '16:08, 6 August 2020': #The code gets completely different after this point. \r\n",
                "        break\r\n",
                "\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "#A rudimentary file which shall be throughighly cleaned later. \r\n",
                "df = pd.DataFrame(complete_list)\r\n",
                "df.to_csv('exchanges_list.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "#Imbetween Part \r\n",
                "# First we have to get all the locations as a list without duplicates. In the future we are going to find the location codes of all these places. We looking everywhere where \r\n",
                "# the locations may be \r\n",
                "list_of_locations = []\r\n",
                "for all_enteries in complete_list:\r\n",
                "    for exchanges in all_enteries[1]['Locations']: #The zero entry would otherwise take us to the date. \r\n",
                "        for exchange_locations in exchanges:\r\n",
                "            if exchange_locations not in list_of_locations: \r\n",
                "                list_of_locations.append(exchange_locations)\r\n",
                "#print(complete_dictionary[0])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['New York City', 'Hong Kong', 'Shanghai', 'Tokyo', 'Osaka', 'Amsterdam', 'Brussels', 'Dublin', 'Lisbon', 'Milan', 'Oslo', 'Paris', 'Shenzhen', 'London', 'Mumbai', 'Toronto', 'Riyadh', 'Frankfurt', 'Seoul', 'Busan', 'Zurich', 'Taipei', 'Sydney', 'Johannesburg', 'SÃ£o Paulo', 'Madrid', 'Singapore', 'Moscow', 'Bangkok', 'Mexico City', 'Jakarta', 'Market place']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Next part: Correlating each location to a code to search for. This cell is about finding the URL's. The next cell gets the code from the URL. \r\n",
                "import time  #We need to do a bit of waiting to make sure everything loads properly on the page we are using. \r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "driver.get(weather_URL) # This is the page in the weather website where you can input the location and date and look for infomation. \r\n",
                "location_urls = []\r\n",
                "for location in list_of_locations:\r\n",
                "    time.sleep(4)\r\n",
                "    search_bar = driver.find_element_by_name('historySearch')   #This is the search bar\r\n",
                "    search_bar.click()\r\n",
                "    if location == 'Hong Kong':\r\n",
                "        location = 'Kowloon'   #This is a nessecity as hong kong does not show up correctly here. \r\n",
                "    if location == 'Mexico City': \r\n",
                "        location = 'IMEXICOC52' # Meixco city works neither\r\n",
                "    search_bar.send_keys(location)  #Type the location in. \r\n",
                "    time.sleep(4)\r\n",
                "    search_bar.send_keys(Keys.RETURN) #Pressing enter after a delay will send you to a page regarding that location. \r\n",
                "    view_button = driver.find_element_by_xpath('/html/body/app-root/app-history-search/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div/div/div/div/form/lib-date-selector/div/input')\r\n",
                "    view_button.click()\r\n",
                "    view_button.click()  #This activate button needs to be clicked twice\r\n",
                "    time.sleep(2)  #The code does not scrape properly without these periods of waiting. \r\n",
                "    current_url = driver.current_url\r\n",
                "    while len(current_url) == 36: #This is the length of the search page url. If it is this long we need to wait more to get a url with a location code in it.\r\n",
                "        time.sleep(1)\r\n",
                "        current_url = driver.current_url\r\n",
                "    location_urls.append(current_url)  #Add the url to the list of urls. These are inputted in the exact same order as the locations in the locations list. \r\n",
                "    time.sleep(2)\r\n",
                "    driver.get(weather_URL)  #Return to searchpage\r\n",
                "driver.quit()\r\n",
                "\r\n",
                "\r\n",
                "#This next part concerns matching each location with the key we can find in the url. \r\n",
                "# No information found for november 2020\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Here we are matching the location to the specalised key that appears in the url of each url when searching for that location. \r\n",
                "\r\n",
                "location_key_match = []\r\n",
                "url_position = 0\r\n",
                "for location in list_of_locations:  #For all locations in this non duplicating list....\r\n",
                "    location_entry = {}\r\n",
                "    location_entry['Location Name'] = location  #add the name of the location here.\r\n",
                "    location_entry['key'] = location_urls[url_position][43:47]   #43 to 47 is the position in the code where the key appears. Also add this to the dictionary.\r\n",
                "    location_key_match.append(location_entry)    #This is a list with all the couplings of location name and key\r\n",
                "    url_position += 1  # Move to the next URL\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#We are now creating coupluings for the month and the location. This, alongside the previous list, really helps us when scraping the weather page by giving us direct links.\r\n",
                "# This tells us which locations are important for each month. \r\n",
                "month_location_match = []\r\n",
                "for months in complete_list:   #For all the month variables in the complete exchange list. \r\n",
                "    all_months_locations = []\r\n",
                "    current_month_location_match = {}\r\n",
                "    for lists in months[1]['Locations']:  #This collection of for loops finds all the locations correpsonding to this month. \r\n",
                "        for items in lists:\r\n",
                "            if items not in all_months_locations: #No duplicates\r\n",
                "                all_months_locations.append(items)\r\n",
                "    current_month_location_match['Month'] = months[0]   # The zero entry here is the name of the month. \r\n",
                "    current_month_location_match['Locations'] = all_months_locations #This dictonary value is the list of all the locations appearing in this month. \r\n",
                "    month_location_match.append(current_month_location_match) #This is a list of these dictionaries\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "#reset\r\n",
                "Giant_dataset = []"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "def trying_scraping(current_date_dict, driver):  # This function tries scraping from the locations on the weather website. These are try statements so if a page has no info, no info is added.\r\n",
                "    try:\r\n",
                "       \r\n",
                "      high_temp = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[1]/td[1]')\r\n",
                "      current_date_dict['High_Temp'] = high_temp.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "      low_temp = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[2]/td[1]')\r\n",
                "      current_date_dict['Low_Temp'] = low_temp.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        day_average = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[3]/td[1]')\r\n",
                "        current_date_dict['Day_Average_Temp'] = day_average.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        precipitation_number = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[2]/tr/td[1]')\r\n",
                "        current_date_dict['Precipitation_Number'] = precipitation_number.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "       dew_point = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[1]/td[1]')\r\n",
                "       current_date_dict['Dew_point'] = dew_point.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        dew_high = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[2]/td[1]')\r\n",
                "        current_date_dict['Dew_High'] = dew_high.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        dew_low = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[3]/td[1]')\r\n",
                "        current_date_dict['Dew_Low'] = dew_low.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "    \r\n",
                "    try:\r\n",
                "        dew_average = driver.find_element_by_xpath('')\r\n",
                "        current_date_dict['Dew_Average'] = dew_average.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    try:\r\n",
                "         max_wind_speed = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[4]/tr[1]/td[1]')\r\n",
                "         current_date_dict['Max_Wind_Speed'] = max_wind_speed.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    try:\r\n",
                "         visibility = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[4]/tr[2]/td[1]')\r\n",
                "         current_date_dict['Visibility'] = visibility.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        day_length = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[6]/tr[1]/td[1]')\r\n",
                "        current_date_dict['Day_length'] = day_length.text\r\n",
                "    except:\r\n",
                "        pass\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "def daily_observations_scraping(current_date_dict, driver):  # This is a fucntion which scrapes from the table in the weather section. \r\n",
                "    try:\r\n",
                "       observations_body = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[5]/div[1]/div/lib-city-history-observation/div/div[2]/table/tbody')\r\n",
                "       list_of_rows = observations_body.find_elements_by_xpath(\"*\")  # These of all the rows of the table. \r\n",
                "       time_data = []\r\n",
                "       for row in list_of_rows:\r\n",
                "           time_condition = {} # A dictionary recording data for each time in the day in the table. \r\n",
                "           row_elements = row.find_elements_by_xpath(\"*\")\r\n",
                "           column_number = 1 # Start with the leftmost column. \r\n",
                "           for column in row_elements:\r\n",
                "               if column_number == 1:\r\n",
                "                   time_condition['Time'] = column.text # This column is the time column. \r\n",
                "               if column_number == 5: \r\n",
                "                   time_condition['Wind_Direction'] = column.text #This column is the wind direction column. \r\n",
                "               if column_number == 10:\r\n",
                "                   time_condition['Rain_Condition'] = column.text   #This column is the worded rain column. \r\n",
                "               column_number += 1\r\n",
                "           time_data.append(time_condition)       #collect all the time data together and add them to the dictionary. \r\n",
                "       current_date_dict['Time_Data'] = time_data\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Now we have month location match and location key match we can web scrap the weather information\r\n",
                "#standard url = https://www.wunderground.com/history/daily/MMMX/date/2021-8-2\r\n",
                "#we are going to go through the months and for all the locations within that month, we are going for each day to each location\r\n",
                "#This dictonary tells us what month corresponds to what number \r\n",
                "month_number_match = [{'Month': 'January', 'Number': '1'},\r\n",
                "                      {'Month': 'February', 'Number': '2'}, \r\n",
                "                      {'Month': 'March', 'Number': '3'}, \r\n",
                "                      {'Month': 'April', 'Number': '4'}, \r\n",
                "                      {'Month': 'May', 'Number': '5'}, \r\n",
                "                      {'Month': 'June', 'Number': '6'},\r\n",
                "                      {'Month': 'July', 'Number': '7'},\r\n",
                "                      {'Month': 'August', 'Number': '8'},\r\n",
                "                      {'Month': 'September', 'Number': '9'},\r\n",
                "                      {'Month': 'October', 'Number': '10'},\r\n",
                "                      {'Month': 'November', 'Number': '11'},\r\n",
                "                      {'Month': 'December', 'Number': '12'},]\r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "for months in month_location_match: #We are iterating through the days, then the locations, then the months. Months is the outer for loop.\r\n",
                "    current_month_dict = {}\r\n",
                "    amount_of_days = 28 #default amount\r\n",
                "    month_number = 0 # Before we know the number of the month, we set it to zero. \r\n",
                "    if months['Month'][1] in ('July', 'May', 'March', 'January', 'December', 'October','August'):  #These months have 31 days\r\n",
                "        amount_of_days = 31\r\n",
                "    elif months['Month'][1] in ('June', 'April', 'November', 'September'): #These months have 30 days\r\n",
                "        amount_of_days = 30\r\n",
                "    elif int(months['Month'][0]) %4 == 0: #A test for whether the year is a leap year or not. This is sufficient logic in the span of dates we are looking at. \r\n",
                "        amount_of_days = 29\r\n",
                "    for month_entry in month_number_match:  #We are going through the dictionary here to find the corresponding month number to each month we are looking through.\r\n",
                "        if month_entry['Month'] == months['Month'][1]:\r\n",
                "            month_number = month_entry['Number']    #We need to do this as the URL uses an numerical input for the month value. \r\n",
                "    current_year = months['Month'][0]\r\n",
                "    current_month_dict['Month'] = (months['Month'][0], months['Month'][1]) #These last two enteries correspond to the month of the year and the year. \r\n",
                "    current_month_dict['Data'] = [] #This is an empty list which would contain all scrapped data for this month. Each entry corresponds to a day and a location. \r\n",
                "    for locations in months['Locations']:  #We are finding the key for each location here.\r\n",
                "        current_key = ''\r\n",
                "        for items in location_key_match:  \r\n",
                "            if items['Location Name'] == locations:\r\n",
                "                current_key = items['key']    #The key is found.\r\n",
                "        for i in range(1, amount_of_days + 1):  #This range means for every day of the month\r\n",
                "            time.sleep(3) \r\n",
                "            current_date_dict = {}\r\n",
                "            current_date_dict['day_and_location'] = {'day': i,'location': locations}  #Input the independent variables into the dictionary\r\n",
                "            desired_url = f\"https://www.wunderground.com/history/daily/{current_key}/date/{current_year}-{month_number}-{i}\"            \r\n",
                "            #This desired url takes us to the webpage we want given the location, year, day and month   \r\n",
                "            driver.get(desired_url)\r\n",
                "            time.sleep(2)\r\n",
                "            trying_scraping(current_date_dict, driver)   #We are using the two previous functions for scraping these pages\r\n",
                "            daily_observations_scraping(current_date_dict, driver)\r\n",
                "            current_month_dict['Data'].append(current_date_dict)   #Add all this date to the data value of this month\r\n",
                "            \r\n",
                "            \r\n",
                "    Giant_dataset.append(current_month_dict)  #Add the month's data to the giant dataset. \r\n",
                "                            \r\n",
                "\r\n",
                "print('Done')\r\n",
                "df_all = pd.DataFrame(Giant_dataset)\r\n",
                "df_all.to_csv('all_weather.csv', index = False, sep= ';')\r\n",
                "#driver.get('https://www.wunderground.com/history/daily/MMMX/date/2021-8-2')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "df_loc_key = pd.DataFrame(location_key_match)  #This is rudimentary saving of the scrapped data \r\n",
                "df_loc_key.to_csv('locationtion_key.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "df_mon_loc = pd.DataFrame(month_location_match) #This is rudimentary saving of the scrapped data \r\n",
                "df_mon_loc.to_csv('month_location.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\r\n",
                "\r\n",
                "df_all = pd.DataFrame(Giant_dataset) #This is rudimentary saving of the scrapped data \r\n",
                "df_all.to_csv('all_weather.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "3637e7ae26a27c14bbda83048a77043c0b5f81d10e59f8cc8febad504d9cebce"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}