{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "#This is the cell where I import everything used in this project. The chromedriver is in the folder. \r\n",
                "from selenium import webdriver\r\n",
                "from selenium.webdriver.common.keys import Keys\r\n",
                "import time\r\n",
                "import pandas as pd"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#List of URL home pages. These were found by a manual search. \r\n",
                "exchange_URL = 'https://en.wikipedia.org/w/index.php?title=List_of_stock_exchanges&action=history'\r\n",
                "weather_URL = 'https://www.wunderground.com/history'\r\n",
                "\r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "complete_list = []  #This list would contain all the data we would get from scraping the wikipedia page. \r\n",
                "driver.get(exchange_URL)\r\n",
                "\r\n",
                "\r\n",
                "#This link changes the amount of links per page from 50 to 500. This is great as it covers our whole range. \r\n",
                "fivehundred_link = driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[4]/a[7]')\r\n",
                "fivehundred_link.click()\r\n",
                "\r\n",
                "\r\n",
                "  #The chromedriver is selected and we are going to the stock exchange URL first. \r\n",
                "\r\n",
                "\r\n",
                "#Here is where I set the parameters \r\n",
                "edit_list = []\r\n",
                "current_year = ''\r\n",
                "current_month = ''\r\n",
                "amount_of_ranks = 16\r\n",
                "list_of_month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "full_edit_list = driver.find_element_by_xpath('//*[@id=\"pagehistory\"]') ##This is the block which contains all the links to the edited pages\r\n",
                "edit_list = full_edit_list.find_elements_by_xpath(\"*\") #This list is all the children of the full_edit_lsit\r\n",
                "completed_months_and_years = [] # A list of  two-entried tuples. It marks all the months and years which have already been scraped. \r\n",
                "change_of_format = False #At some point, there is a change in format in the wikipedia page. When this turns true, the code is thereby altered. \r\n",
                "for link in edit_list:\r\n",
                "    month_and_year = []\r\n",
                "    written_date = link.find_element_by_class_name('mw-changeslist-date') # The text here gives us the date\r\n",
                "    written_date_link = written_date.get_attribute('href')\r\n",
                "    written_date_text = written_date.text\r\n",
                "    if written_date_text == '07:40, 24 January 2021': #This link is broken. It must be skipped\r\n",
                "        continue\r\n",
                "    #The following piece of code concerns with deciphering the link's date\r\n",
                "\r\n",
                "\r\n",
                "    current_year = written_date_text[-4:]    #The last four characters of the string always indicates its year.\r\n",
                "    for month in list_of_month_names: #This goes through the actual months of the year and checks whether it is in the string or not. \r\n",
                "        if month in written_date_text:\r\n",
                "            current_month = month #If the month is in the string, then the edit was made on that month of the year.\r\n",
                "    month_and_year = (current_year, current_month) #The first entry of this list is the month of the edit and the second entry is the year.\r\n",
                "\r\n",
                "\r\n",
                "    if written_date_text == '02:17, 22 April 2021': # This is how far we are going back as bugs start arising further back in the past. \r\n",
                "        change_of_format = True\r\n",
                "    if month_and_year not in completed_months_and_years: #If the month has not been scrapped yet...This is to not have multiple datasets per month.\r\n",
                "        this_months_dictonary = {}\r\n",
                "        months_rank_list = []\r\n",
                "        months_exchange_list = []\r\n",
                "        months_location_list = []\r\n",
                "        months_market_cap_list = []\r\n",
                "        months_trade_volume_list = []\r\n",
                "        second_driver = webdriver.Chrome('./chromedriver') # It makes it easier if we use a second driver to open links to a new window. \r\n",
                "        second_driver.get(written_date_link)\r\n",
                "\r\n",
                "        #The code from here to the for loop gets the table as a list of rows. This makes it easier to scrape from \r\n",
                "        table_header = second_driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div[1]/table[2]/tbody/tr[1]')\r\n",
                "        table_header_children =table_header.find_elements_by_xpath(\"*\")\r\n",
                "        current_column = 0 \r\n",
                "        if change_of_format == False:\r\n",
                "            page_table = second_driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody')\r\n",
                "        else:\r\n",
                "            page_table = second_driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody')\r\n",
                "        list_of_rows = page_table.find_elements_by_xpath(\"*\")\r\n",
                "        for row in list_of_rows:    #For every row in the table\r\n",
                "            list_of_row_elements = row.find_elements_by_xpath(\"*\")\r\n",
                "            k = 1   #This k indicates which column we are concerned with\r\n",
                "            for row_elements in list_of_row_elements:\r\n",
                "                if len(list_of_row_elements) == 16:  #This is the correct amount of elements in a row. Any other number than this makes it difficult to scrape from. \r\n",
                "                    if k == 1:  #This column corresponds to the rank of the exchange. \r\n",
                "                        months_rank_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k ==3: #This column corresponds to the name of the exchange.\r\n",
                "                        months_exchange_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k == 6:\r\n",
                "                        split_locations = row_elements.text.split('\\n')\r\n",
                "                        months_location_list.append(split_locations)\r\n",
                "\r\n",
                "                    if k == 7:  #This column corresponds to the market's market cap. \r\n",
                "                        months_market_cap_list.append(row_elements.text)\r\n",
                "\r\n",
                "                    if k ==8: # This column corresponds to the market's trade volume\r\n",
                "                        months_trade_volume_list.append(row_elements.text)\r\n",
                "                k += 1 #  Move to the next column \r\n",
                "                \r\n",
                "                    \r\n",
                "        #Here we are adding the information we have scrapped to this months dictionary. \r\n",
                "        this_months_dictonary['Rank'] = months_rank_list\r\n",
                "        this_months_dictonary['Name of exchange'] = months_exchange_list\r\n",
                "        this_months_dictonary['Locations'] = months_location_list\r\n",
                "        this_months_dictonary['Market Cap'] = months_market_cap_list\r\n",
                "        this_months_dictonary['Trade Voume'] = months_trade_volume_list\r\n",
                "        complete_list.append([month_and_year, this_months_dictonary])                      \r\n",
                "        second_driver.quit()\r\n",
                "        completed_months_and_years.append(month_and_year)\r\n",
                "    \r\n",
                "    if written_date_text == '16:08, 6 August 2020': #The code gets completely different after this point. \r\n",
                "        break\r\n",
                "\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "#A rudimentary file which shall be throughighly cleaned later. \r\n",
                "df = pd.DataFrame(complete_list)\r\n",
                "df.to_csv('exchanges_list.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "#Imbetween Part \r\n",
                "# First we have to get all the locations as a list without duplicates. In the future we are going to find the location codes of all these places. We looking everywhere where \r\n",
                "# the locations may be \r\n",
                "list_of_locations = []\r\n",
                "for all_enteries in complete_list:\r\n",
                "    for exchanges in all_enteries[1]['Locations']: #The zero entry would otherwise take us to the date. \r\n",
                "        for exchange_locations in exchanges:\r\n",
                "            if exchange_locations not in list_of_locations: \r\n",
                "                list_of_locations.append(exchange_locations)\r\n",
                "#print(complete_dictionary[0])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['New York City', 'Hong Kong', 'Shanghai', 'Tokyo', 'Osaka', 'Amsterdam', 'Brussels', 'Dublin', 'Lisbon', 'Milan', 'Oslo', 'Paris', 'Shenzhen', 'London', 'Mumbai', 'Toronto', 'Riyadh', 'Frankfurt', 'Seoul', 'Busan', 'Zurich', 'Taipei', 'Sydney', 'Johannesburg', 'São Paulo', 'Madrid', 'Singapore', 'Moscow', 'Bangkok', 'Mexico City', 'Jakarta', 'Market place']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Next part: Correlating each location to a code to search for. This cell is about finding the URL's. The next cell gets the code from the URL. \r\n",
                "import time  #We need to do a bit of waiting to make sure everything loads properly on the page we are using. \r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "driver.get(weather_URL) # This is the page in the weather website where you can input the location and date and look for infomation. \r\n",
                "location_urls = []\r\n",
                "for location in list_of_locations:\r\n",
                "    time.sleep(4)\r\n",
                "    search_bar = driver.find_element_by_name('historySearch')   #This is the search bar\r\n",
                "    search_bar.click()\r\n",
                "    if location == 'Hong Kong':\r\n",
                "        location = 'Kowloon'   #This is a nessecity as hong kong does not show up correctly here. \r\n",
                "    if location == 'Mexico City': \r\n",
                "        location = 'IMEXICOC52' # Meixco city works neither\r\n",
                "    search_bar.send_keys(location)  #Type the location in. \r\n",
                "    time.sleep(4)\r\n",
                "    search_bar.send_keys(Keys.RETURN) #Pressing enter after a delay will send you to a page regarding that location. \r\n",
                "    view_button = driver.find_element_by_xpath('/html/body/app-root/app-history-search/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div/div/div/div/form/lib-date-selector/div/input')\r\n",
                "    view_button.click()\r\n",
                "    view_button.click()  #This activate button needs to be clicked twice\r\n",
                "    time.sleep(2)  #The code does not scrape properly without these periods of waiting. \r\n",
                "    current_url = driver.current_url\r\n",
                "    while len(current_url) == 36: #This is the length of the search page url. If it is this long we need to wait more to get a url with a location code in it.\r\n",
                "        time.sleep(1)\r\n",
                "        current_url = driver.current_url\r\n",
                "    location_urls.append(current_url)  #Add the url to the list of urls. These are inputted in the exact same order as the locations in the locations list. \r\n",
                "    time.sleep(2)\r\n",
                "    driver.get(weather_URL)  #Return to searchpage\r\n",
                "driver.quit()\r\n",
                "\r\n",
                "\r\n",
                "#This next part concerns matching each location with the key we can find in the url. \r\n",
                "# No information found for november 2020\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Here we are matching the location to the specalised key that appears in the url of each url when searching for that location. \r\n",
                "\r\n",
                "location_key_match = []\r\n",
                "url_position = 0\r\n",
                "for location in list_of_locations:  #For all locations in this non duplicating list....\r\n",
                "    location_entry = {}\r\n",
                "    location_entry['Location Name'] = location  #add the name of the location here.\r\n",
                "    location_entry['key'] = location_urls[url_position][43:47]   #43 to 47 is the position in the code where the key appears. Also add this to the dictionary.\r\n",
                "    location_key_match.append(location_entry)    #This is a list with all the couplings of location name and key\r\n",
                "    url_position += 1  # Move to the next URL\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#We are now creating coupluings for the month and the location. This, alongside the previous list, really helps us when scraping the weather page by giving us direct links.\r\n",
                "# This tells us which locations are important for each month. \r\n",
                "month_location_match = []\r\n",
                "for months in complete_list:   #For all the month variables in the complete exchange list. \r\n",
                "    all_months_locations = []\r\n",
                "    current_month_location_match = {}\r\n",
                "    for lists in months[1]['Locations']:  #This collection of for loops finds all the locations correpsonding to this month. \r\n",
                "        for items in lists:\r\n",
                "            if items not in all_months_locations: #No duplicates\r\n",
                "                all_months_locations.append(items)\r\n",
                "    current_month_location_match['Month'] = months[0]   # The zero entry here is the name of the month. \r\n",
                "    current_month_location_match['Locations'] = all_months_locations #This dictonary value is the list of all the locations appearing in this month. \r\n",
                "    month_location_match.append(current_month_location_match) #This is a list of these dictionaries\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "#reset\r\n",
                "Giant_dataset = []"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "def trying_scraping(current_date_dict, driver):  # This function tries scraping from the locations on the weather website. These are try statements so if a page has no info, no info is added.\r\n",
                "    try:\r\n",
                "       \r\n",
                "      high_temp = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[1]/td[1]')\r\n",
                "      current_date_dict['High_Temp'] = high_temp.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "      low_temp = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[2]/td[1]')\r\n",
                "      current_date_dict['Low_Temp'] = low_temp.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        day_average = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[1]/tr[3]/td[1]')\r\n",
                "        current_date_dict['Day_Average_Temp'] = day_average.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        precipitation_number = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[2]/tr/td[1]')\r\n",
                "        current_date_dict['Precipitation_Number'] = precipitation_number.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "       dew_point = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[1]/td[1]')\r\n",
                "       current_date_dict['Dew_point'] = dew_point.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        dew_high = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[2]/td[1]')\r\n",
                "        current_date_dict['Dew_High'] = dew_high.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        dew_low = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[3]/tr[3]/td[1]')\r\n",
                "        current_date_dict['Dew_Low'] = dew_low.text\r\n",
                "    except:\r\n",
                "       pass\r\n",
                "    \r\n",
                "    try:\r\n",
                "        dew_average = driver.find_element_by_xpath('')\r\n",
                "        current_date_dict['Dew_Average'] = dew_average.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    try:\r\n",
                "         max_wind_speed = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[4]/tr[1]/td[1]')\r\n",
                "         current_date_dict['Max_Wind_Speed'] = max_wind_speed.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    try:\r\n",
                "         visibility = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[4]/tr[2]/td[1]')\r\n",
                "         current_date_dict['Visibility'] = visibility.text\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    try:\r\n",
                "        day_length = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[3]/div[1]/div/lib-city-history-summary/div/div[2]/table/tbody[6]/tr[1]/td[1]')\r\n",
                "        current_date_dict['Day_length'] = day_length.text\r\n",
                "    except:\r\n",
                "        pass\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "def daily_observations_scraping(current_date_dict, driver):  # This is a fucntion which scrapes from the table in the weather section. \r\n",
                "    try:\r\n",
                "       observations_body = driver.find_element_by_xpath('/html/body/app-root/app-history/one-column-layout/wu-header/sidenav/mat-sidenav-container/mat-sidenav-content/div/section/div[2]/div[1]/div[5]/div[1]/div/lib-city-history-observation/div/div[2]/table/tbody')\r\n",
                "       list_of_rows = observations_body.find_elements_by_xpath(\"*\")  # These of all the rows of the table. \r\n",
                "       time_data = []\r\n",
                "       for row in list_of_rows:\r\n",
                "           time_condition = {} # A dictionary recording data for each time in the day in the table. \r\n",
                "           row_elements = row.find_elements_by_xpath(\"*\")\r\n",
                "           column_number = 1 # Start with the leftmost column. \r\n",
                "           for column in row_elements:\r\n",
                "               if column_number == 1:\r\n",
                "                   time_condition['Time'] = column.text # This column is the time column. \r\n",
                "               if column_number == 5: \r\n",
                "                   time_condition['Wind_Direction'] = column.text #This column is the wind direction column. \r\n",
                "               if column_number == 10:\r\n",
                "                   time_condition['Rain_Condition'] = column.text   #This column is the worded rain column. \r\n",
                "               column_number += 1\r\n",
                "           time_data.append(time_condition)       #collect all the time data together and add them to the dictionary. \r\n",
                "       current_date_dict['Time_Data'] = time_data\r\n",
                "    except:\r\n",
                "        pass\r\n",
                "\r\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Now we have month location match and location key match we can web scrap the weather information\r\n",
                "#standard url = https://www.wunderground.com/history/daily/MMMX/date/2021-8-2\r\n",
                "#we are going to go through the months and for all the locations within that month, we are going for each day to each location\r\n",
                "#This dictonary tells us what month corresponds to what number \r\n",
                "month_number_match = [{'Month': 'January', 'Number': '1'},\r\n",
                "                      {'Month': 'February', 'Number': '2'}, \r\n",
                "                      {'Month': 'March', 'Number': '3'}, \r\n",
                "                      {'Month': 'April', 'Number': '4'}, \r\n",
                "                      {'Month': 'May', 'Number': '5'}, \r\n",
                "                      {'Month': 'June', 'Number': '6'},\r\n",
                "                      {'Month': 'July', 'Number': '7'},\r\n",
                "                      {'Month': 'August', 'Number': '8'},\r\n",
                "                      {'Month': 'September', 'Number': '9'},\r\n",
                "                      {'Month': 'October', 'Number': '10'},\r\n",
                "                      {'Month': 'November', 'Number': '11'},\r\n",
                "                      {'Month': 'December', 'Number': '12'},]\r\n",
                "driver = webdriver.Chrome('./chromedriver')\r\n",
                "for months in month_location_match: #We are iterating through the days, then the locations, then the months. Months is the outer for loop.\r\n",
                "    current_month_dict = {}\r\n",
                "    amount_of_days = 28 #default amount\r\n",
                "    month_number = 0 # Before we know the number of the month, we set it to zero. \r\n",
                "    if months['Month'][1] in ('July', 'May', 'March', 'January', 'December', 'October','August'):  #These months have 31 days\r\n",
                "        amount_of_days = 31\r\n",
                "    elif months['Month'][1] in ('June', 'April', 'November', 'September'): #These months have 30 days\r\n",
                "        amount_of_days = 30\r\n",
                "    elif int(months['Month'][0]) %4 == 0: #A test for whether the year is a leap year or not. This is sufficient logic in the span of dates we are looking at. \r\n",
                "        amount_of_days = 29\r\n",
                "    for month_entry in month_number_match:  #We are going through the dictionary here to find the corresponding month number to each month we are looking through.\r\n",
                "        if month_entry['Month'] == months['Month'][1]:\r\n",
                "            month_number = month_entry['Number']    #We need to do this as the URL uses an numerical input for the month value. \r\n",
                "    current_year = months['Month'][0]\r\n",
                "    current_month_dict['Month'] = (months['Month'][0], months['Month'][1]) #These last two enteries correspond to the month of the year and the year. \r\n",
                "    current_month_dict['Data'] = [] #This is an empty list which would contain all scrapped data for this month. Each entry corresponds to a day and a location. \r\n",
                "    for locations in months['Locations']:  #We are finding the key for each location here.\r\n",
                "        current_key = ''\r\n",
                "        for items in location_key_match:  \r\n",
                "            if items['Location Name'] == locations:\r\n",
                "                current_key = items['key']    #The key is found.\r\n",
                "        for i in range(1, amount_of_days + 1):  #This range means for every day of the month\r\n",
                "            time.sleep(3) \r\n",
                "            current_date_dict = {}\r\n",
                "            current_date_dict['day_and_location'] = {'day': i,'location': locations}  #Input the independent variables into the dictionary\r\n",
                "            desired_url = f\"https://www.wunderground.com/history/daily/{current_key}/date/{current_year}-{month_number}-{i}\"            \r\n",
                "            #This desired url takes us to the webpage we want given the location, year, day and month   \r\n",
                "            driver.get(desired_url)\r\n",
                "            time.sleep(2)\r\n",
                "            trying_scraping(current_date_dict, driver)   #We are using the two previous functions for scraping these pages\r\n",
                "            daily_observations_scraping(current_date_dict, driver)\r\n",
                "            current_month_dict['Data'].append(current_date_dict)   #Add all this date to the data value of this month\r\n",
                "            \r\n",
                "            \r\n",
                "    Giant_dataset.append(current_month_dict)  #Add the month's data to the giant dataset. \r\n",
                "                            \r\n",
                "\r\n",
                "print('Done')\r\n",
                "df_all = pd.DataFrame(Giant_dataset)\r\n",
                "df_all.to_csv('all_weather.csv', index = False, sep= ';')\r\n",
                "#driver.get('https://www.wunderground.com/history/daily/MMMX/date/2021-8-2')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "df_loc_key = pd.DataFrame(location_key_match)  #This is rudimentary saving of the scrapped data \r\n",
                "df_loc_key.to_csv('locationtion_key.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "df_mon_loc = pd.DataFrame(month_location_match) #This is rudimentary saving of the scrapped data \r\n",
                "df_mon_loc.to_csv('month_location.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\r\n",
                "\r\n",
                "df_all = pd.DataFrame(Giant_dataset) #This is rudimentary saving of the scrapped data \r\n",
                "df_all.to_csv('all_weather.csv', index = False, sep= ';')"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "interpreter": {
            "hash": "3637e7ae26a27c14bbda83048a77043c0b5f81d10e59f8cc8febad504d9cebce"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}